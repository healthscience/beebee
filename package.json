{
  "name": "beebee",
  "version": "0.1.0",
  "description": "BeeBee LLM service using node-llama-cpp",
  "main": "src/index.js",
  "type": "module",
  "scripts": {
    "start": "node examples/basic.js",
    "server": "node examples/server.js",
    "client": "node examples/client.js",
    "test": "vitest",
    "test:ui": "vitest --ui",
    "test:coverage": "vitest --coverage",
    "dev": "node --watch examples/basic.js",
    "dev:server": "node --watch examples/server.js"
  },
  "dependencies": {
    "node-llama-cpp": "^3.0.0",
    "express": "^4.18.2",
    "ws": "^8.14.2",
    "cors": "^2.8.5",
    "node-fetch": "^3.3.2"
  },
  "devDependencies": {
    "@types/node": "^20.0.0",
    "vitest": "^1.0.0",
    "@vitest/ui": "^1.0.0",
    "@vitest/coverage-v8": "^1.0.0"
  },
  "engines": {
    "node": ">=18.0.0"
  },
  "keywords": [
    "llm",
    "llama",
    "ai",
    "beebee"
  ],
  "author": "",
  "license": "GPL-3.0"
}